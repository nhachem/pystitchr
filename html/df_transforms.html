<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>df_transforms API documentation</title>
<meta name="description" content="pyspark â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>df_transforms</code></h1>
</header>
<section id="section-intro">
<p>pyspark</p>
<p>from stitchr_extensions.df_transforms import *</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
pyspark

from stitchr_extensions.df_transforms import *

&#34;&#34;&#34;

# import os
# from pyspark.sql.functions import concat_ws, collect_list
# import typing

import pyspark
from pyspark import rdd
from pyspark.sql.types import *
from pyspark.sql.functions import col, concat, lit, when
from pyspark.sql.dataframe import DataFrame
import pyspark.sql.functions as F
from pyspark.sql.functions import expr
# import typing_extensions

# import sys
from random import choice
from string import ascii_letters
import re

from typing import List

spark = (pyspark.sql.SparkSession.builder.getOrCreate())
spark.sparkContext.setLogLevel(&#39;WARN&#39;)
&#34;&#34;&#34;
dict structures that we can get from data catalogs
&#34;&#34;&#34;


to_spark_type_dict_by_string: dict = {
    &#34;string&#34;: &#34;StringType&#34;,
    &#34;Char&#34;: &#34;StringType&#34;,
    &#34;Datetime&#34;: &#34;TimestampType&#34;,
    &#34;Duration&#34;: &#34;FloatType&#34;,
    &#34;Double&#34;: &#34;DoubleType&#34;,
    &#34;number&#34;: &#34;DoubleType&#34;,
    # &#34;Num&#34;: &#34;FloatType&#34;,
    &#34;Num&#34;: &#34;DoubleType&#34;,
    &#34;float&#34;: &#34;FloatType&#34;,
    &#34;integer&#34;: &#34;LongType&#34;,
    &#34;Int&#34;: &#34;LongType&#34;,
    &#34;boolean&#34;: &#34;BooleanType&#34;,
    &#34;Boolean&#34;: &#34;BooleanType&#34;,
    &#34;None&#34;: &#34;NullType&#34;,
    &#34;Unknown&#34;: &#34;StringType&#34;
}

cast_to_spark_type_dict = {
    &#34;string&#34;: &#34;string&#34;,
    &#34;Char&#34;: &#34;string&#34;,
    &#34;Datetime&#34;: &#34;timestamp&#34;,
    &#34;Duration&#34;: &#34;float&#34;,
    &#34;number&#34;: &#34;double&#34;,
    # &#34;Num&#34;: &#34;FloatType&#34;,
    &#34;Num&#34;: &#34;double&#34;,
    &#34;float&#34;: &#34;float&#34;,
    &#34;integer&#34;: &#34;long&#34;,
    &#34;Int&#34;: &#34;long&#34;,
    &#34;boolean&#34;: &#34;boolean&#34;,
    &#34;Boolean&#34;: &#34;boolean&#34;,
    &#34;None&#34;: &#34;string&#34;,
    &#34;Unknown&#34;: &#34;string&#34;
}

&#34;&#34;&#34;spark = SparkSession.builder.getOrCreate()
# import spark.implicits._
spark.sparkContext.setLogLevel(&#39;WARN&#39;)
&#34;&#34;&#34;
# maybe add to logging print(sys.path)

&#34;&#34;&#34; setting up the path to include Stitchr and other project related imports&#34;&#34;&#34;

# sys.path.append(os.environ[&#39;STITCHR_ROOT&#39;] + &#39;/pyspark-app/app&#39;)

# print(&#34;Spark Version:&#34;, spark.sparkContext.version)

&#34;&#34;&#34;schema generation code, including missing columns 
&#34;&#34;&#34;
&#34;&#34;&#34;
assumes we have the table schema_metadata
we could pass the columns as a list and then convert tin the function
Need to add a catch all StringType() to the map
&#34;&#34;&#34;


# using call by string name getattr()
def generate_schema_by_string(domain: str, columns: list, attributes_df: DataFrame):
    import pyspark.sql.types as t
    filter_string = &#34;&#39;,&#39;&#34;.join(columns)
    col_df = spark.createDataFrame(columns, StringType())

    column_meta_df = attributes_df \
        .filter(f&#34;domain_prefix = &#39;{domain}&#39;&#34;) \
        .select(&#34;sequence&#34;, &#34;variable_name&#34;, &#34;datatype&#34;, &#34;core&#34;)
    # NH need to test that the order is conserved...
    meta_df = col_df.join(column_meta_df, col_df.value == column_meta_df.variable_name, &#34;left&#34;) \
        .drop(&#34;variable_name&#34;) \
        .withColumnRenamed(&#34;value&#34;, &#34;variable_name&#34;) \
        .withColumn(&#34;datatype&#34;, when(column_meta_df.datatype.isNull(), &#34;Unknown&#34;)
                    .otherwise(column_meta_df.datatype))

    # meta_df.show(50, False)
    # meta_df.printSchema()
    column_meta = meta_df.collect()
    print(column_meta)
    m = list(map(lambda column: StructField(column.variable_name,
                                            getattr(t, to_spark_type_dict_by_string[column.datatype])(), True),
                 column_meta))
    return StructType(m)


def generate_missing_columns(domain: str, columns: list, attributes_df: DataFrame) -&gt; list:
    # using call by string name getattr()
    col_df = spark.createDataFrame(columns, StringType())
    column_meta_df = attributes_df \
        .filter(f&#34;domain_prefix = &#39;{domain}&#39;&#34;) \
        .select(&#34;sequence&#34;, &#34;variable_name&#34;, &#34;datatype&#34;, &#34;core&#34;)
    # NH need to test that the order is conserved...
    meta_df = col_df.join(column_meta_df, col_df.value == column_meta_df.variable_name, &#34;right&#34;) \
        .filter(&#34;value is null&#34;).orderBy(&#34;sequence&#34;)

    # meta_df.show(50, False)
    # meta_df.printSchema()
    column_meta = meta_df.collect()
    print(column_meta)
    m = list(map(lambda column: (column.sequence, column.variable_name, cast_to_spark_type_dict.get(column.datatype)),
                 column_meta))
    return m


def left_diff_schemas(left_df: DataFrame, right_df: DataFrame) -&gt; list:
    &#34;&#34;&#34;

    :param left_df:
    :param right_df:
    :return:
    &#34;&#34;&#34;
    left_columns_set = set(left_df.schema.fieldNames())
    right_columns_set = set(right_df.schema.fieldNames())
    # what is the toSet on python? .toSet
    # print(list(df_columns_set))
    # warn that some columns are not in the list... Or maybe throw an error?
    return list(left_columns_set - right_columns_set)


def right_diff_schemas(left_df: DataFrame, right_df: DataFrame) -&gt; list:
    &#34;&#34;&#34;

    :param left_df:
    :param right_df:
    :return:
    &#34;&#34;&#34;
    left_columns_set = set(left_df.schema.names)
    right_columns_set = set(right_df.schema.names)
    # what is the toSet on python? .toSet
    # print(list(df_columns_set))
    # warn that some columns are not in the list... Or maybe throw an error?
    return list(right_columns_set - left_columns_set)


# modify to test nested and also use set operations left.diff(right)?
# look into panda equivalent or maybe quoalas
def schema_diff(left_df: DataFrame, right_df: DataFrame):
    right_columns_set = set(right_df.schema)
    left_columns_set = set(left_df.schema)
    return ([l for l in left_df.schema if l not in right_columns_set],
            [r for r in right_df.schema if r not in left_columns_set]
            )


# to add to pystitchr
def fields_diff(left_df: DataFrame, right_df: DataFrame):
    l_set = set(left_df.schema.fieldNames())
    r_set = set(right_df.schema.fieldNames())
    return l_set.difference(r_set), r_set.difference(l_set)


def select_list(df: DataFrame, column_list: list) -&gt; DataFrame:
    &#34;&#34;&#34;

    :param df:
    :param column_list:
    :return:
    &#34;&#34;&#34;
    cl: list = f&#34;`{&#39;`,`&#39;.join(column_list)}`&#34;.split(&#39;,&#39;)
    return df.select(*cl)


def select_exclude(df: DataFrame, columns_2_exclude: list) -&gt; DataFrame:
    &#34;&#34;&#34;
    :param df:
    :param columns_2_exclude:
    :return:
    &#34;&#34;&#34;
    column_list: list = list(set(df.schema.fieldNames()) - set(columns_2_exclude))
    return df.select(*column_list)


def drop_columns(df: DataFrame, drop_columns_list: list) -&gt; DataFrame:
    &#34;&#34;&#34;

    :param drop_columns_list:
    :param df:
    :return:
    &#34;&#34;&#34;
    df_columns_set = set(df.schema.fieldNames())
    # warn that some columns are not in the list... Or maybe throw an error?
    cols_that_do_not_exist = set(drop_columns_list) - df_columns_set
    # get the actual list of columns to drop
    columns_2_remove = list(set(drop_columns_list) - cols_that_do_not_exist)
    # drop and return
    return df.drop(*columns_2_remove)


# too long as a line need to figure out how to wrap the lambda function in multi-line code?
&#34;&#34;&#34;
def drop_columns(drop_columns_list: list, df: DataFrame) -&gt; DataFrame:
    
    return df.drop(*list(set(drop_columns_list) - (set(drop_columns_list) - set(df.schema.fieldNames()))))

&#34;&#34;&#34;


def rename_columns(df: DataFrame, rename_mapping_dict: dict) -&gt; DataFrame:
    &#34;&#34;&#34;
    Caveat: NH: need to fix. The outcome changes the type to string
    :param df:
    :param rename_mapping_dict:
    :return: DataFrame
    Takes a dictionary of columns to be renamed and returns a converted dataframe
    &#34;&#34;&#34;
    # we use iteration over the dict and call df.WithColumn
    # this is not efficient?
    df_columns: list = df.schema.fieldNames()
    df_new_columns: list = [rename_mapping_dict[c] if (c in rename_mapping_dict)
                            else c
                            for c in df_columns]
    # df_new_columns: list = [f&#34;`{rename_mapping_dict[c]}`&#34; if (c in rename_mapping_dict)
    #                         else c
    #                         for c in df_columns]
    return df.toDF(*df_new_columns)


# def rename_column(existing: str, new: str, df: DataFrame) -&gt; DataFrame:
def rename_column(df: DataFrame, existing: str, new: str) -&gt; DataFrame:
    &#34;&#34;&#34;Returns a new :class:`DataFrame` by renaming an existing column.
    This is a no-op if schema doesn&#39;t contain the given column name.
    Effectively a wrapper over withColumnRenamed

    :param df:
    :rtype: object
    :param existing: string, name of the existing column to rename.
    :param new: string, new name of the column.

    &gt;&gt;&gt; df.rename_column(&#39;age&#39;, &#39;age2&#39;).collect()
    [Row(age2=2, name=u&#39;Alice&#39;), Row(age2=5, name=u&#39;Bob&#39;)]
    &#34;&#34;&#34;
    return df.withColumnRenamed(existing, new)


def rename_4_parquet(df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    rename all columns of the dataFrame so that we can save as a Parquet file
    :param df:
    :return:
    &#34;&#34;&#34;
    # need to add left/right trims and replace multiple __ with one?
    # r = &#34;[ ,;{}()\n\t=]&#34;
    # added &#34;.&#34; and &#34;-&#34; and / so that we skip using ``
    # regex = r&#34;[ ,;{}()\n\t=.-]&#34;
    regex = r&#34;[- ,;{}()\n\t=./]&#34;
    delimiter = &#39;__&#39;
    schema_fields = df.schema.fields
    return spark.createDataFrame(
        df.rdd,
        StructType(
            # [StructField(re.sub(regex, delimiter, sf.name.replace(&#39; &#39;, &#39;__&#39;)), sf.dataType, sf.nullable) for sf in
            [StructField(re.sub(regex, delimiter, sf.name.replace(&#39; &#39;, &#39;&#39;)), sf.dataType, sf.nullable) for sf in
             schema_fields]
        )
    )


def rename_4_parquet_p(df: DataFrame, dummy_list: list = [None]) -&gt; DataFrame:
    return rename_4_parquet(df)


def unpivot(df: DataFrame, unpivot_keys: list,
            unpivot_column_list: list,
            key_column: str = &#34;key_column&#34;,
            value_column: str = &#34;value&#34;) -&gt; DataFrame:
    &#34;&#34;&#34;

    :param df:
    :param unpivot_keys:
    :param unpivot_column_list:
    :param key_column:
    :param value_column:
    :return:
    &#34;&#34;&#34;
    # we can improve by checking the parameter lists to be in the schema
    stack_fields_array = unpivot_column_list
    # we need to cast to STRING as we may have int, double , etc... we would couple this with extracting the types
    pivot_map_list = [f&#34;&#39;{s.replace(&#39;`&#39;, &#39;&#39;)}&#39;, STRING(`{s}`) &#34; for s in stack_fields_array]
    stack_fields: str = f&#34;stack({len(stack_fields_array)},{&#39;,&#39;.join([str(x) for x in pivot_map_list])})&#34;
    df.createOrReplaceTempView(&#39;_unpivot&#39;)
    q = f&#34;select `{&#39;`,`&#39;.join([str(x) for x in unpivot_keys])}`, {stack_fields} as (`{key_column}`, `{value_column}`) &#34; \
        f&#34;from _unpivot&#34;
    # replace with logging ... print(f&#39;&#39;&#39;query is: {q} \n&#39;&#39;&#39;)
    return spark.sql(q)


def unpivot_p(df: DataFrame, params: list) -&gt; DataFrame:
    _keys = params[0]
    _unpivot_list = params[1]
    if len(_keys) == 0:
        _keys = list(set(df.schema.fieldNames()).difference(set(_unpivot_list)))
    return unpivot(df, _keys, _unpivot_list)


def flatten0(data_frame: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    NH: Experimental
    :param data_frame:
    :return:
    &#34;&#34;&#34;
    fields: List[StructField] = data_frame.schema.fields
    field_names: list = data_frame.schema.fieldNames()
    # exploded_df = data_frame
    for index, value in enumerate(fields):
        field = value
        field_type = field.dataType
        field_name = field.name
        # print(f&#39;{field}, {field_name}, {field_type}&#39;)
        # we have the case of MapTYpe to handle or isinstance(field_type, MapType)):
        # this means when we see a map we treat as array and add key/value columns?!
        if isinstance(field_type, ArrayType):
            field_names_excluding_array = [fn for fn in field_names if fn != field_name]
            field_names_to_select = field_names_excluding_array + [
                f&#34;explode_outer({field_name}) as {field_name}&#34;]
            # exploded_df = exploded_df.selectExpr(*field_names_to_select)
            # return flatten0(exploded_df)
            return flatten0(data_frame.selectExpr(*field_names_to_select))
        elif isinstance(field_type, MapType):
            &#34;&#34;&#34;
            This is quite expensive if we do not have a known enumeration of key. 
            From https://stackoverflow.com/questions/52762487/flattening-maptype-column-in-pyspark
            df.withColumn(&#34;id&#34;, f.monotonically_increasing_id())\
            .select(&#34;id&#34;, f.explode(&#34;a&#34;))\
            .groupby(&#34;id&#34;)\
            .pivot(&#34;key&#34;)\
            .agg(f.first(&#34;value&#34;))\
            .drop(&#34;id&#34;)\
            In this case, we need to create an id column first so that there&#39;s something to group by.
            The pivot here can be expensive, depending on the size of your data.
            &#34;&#34;&#34;
            &#34;&#34;&#34;
            This solution here outputs a cartesian on each mapped field... 
            Using a pivot like above is better but very expensive
            posexplode does not work so we add an increasing id that we can control
            &#34;&#34;&#34;
            df_mapped = data_frame \
                .withColumn(&#34;id&#34;, F.monotonically_increasing_id()) \
                .select(&#39;*&#39;, F.explode(field_name)) \
                .withColumnRenamed(&#34;key&#34;, f&#34;{field_name}__key_column&#34;) \
                .withColumnRenamed(&#34;value&#34;, f&#34;{field_name}__value&#34;) \
                .withColumnRenamed(&#34;id&#34;, f&#34;{field_name}__id&#34;).drop(field_name)

            return flatten0(df_mapped)
            # return data_frame
        elif isinstance(field_type, StructType):
            child_fieldnames = [f&#34;{field_name}.{child.name}&#34; for child in field_type]
            new_fieldnames = [fn for fn in field_names if fn != field_name] + child_fieldnames
            renamed_cols = [col(x).alias(x.replace(&#34;.&#34;, &#34;__&#34;)) for x in new_fieldnames]
            # exploded_df = exploded_df.select(*renamed_cols)
            # print(len(exploded_df.schema.fieldNames()))
            # return flatten0(exploded_df)
            return flatten0(data_frame.select(*renamed_cols))
    # print(f&#34;schema size is {len(data_frame.schema.fieldNames())}&#34;)
    return data_frame


def flatten_no_explode(data_frame: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    NH: experimental ...still under test.... may explode single element arrays (which may also be acceptable)
    :param data_frame:
    :return:
    &#34;&#34;&#34;
    fields: List[StructField] = data_frame.schema.fields
    field_names: list = data_frame.schema.fieldNames()
    # print(len(field_names))
    # exploded_df = data_frame
    for index, value in enumerate(fields):
        field = value
        field_type = field.dataType
        field_name = field.name
        # print(f&#39;{field}, {field_name}, {field_type}&#39;)
        if isinstance(field_type, StructType):
            child_fieldnames = [f&#34;{field_name}.{child.name}&#34; for child in field_type]
            print(f&#39;{field_name}, {child_fieldnames}&#39;)
            new_fieldnames = [fn for fn in field_names if fn != field_name] + child_fieldnames
            renamed_cols = [col(x).alias(x.replace(&#34;.&#34;, &#34;__&#34;)) for x in new_fieldnames]
            # exploded_df = exploded_df.select(*renamed_cols)
            # print(len(exploded_df.schema.fieldNames()))
            # exploded_df.printSchema()
            # return flatten_no_explode(exploded_df)
            return flatten_no_explode(data_frame.select(*renamed_cols))
    # print(f&#39;schema size is {len(data_frame.schema.fieldNames())}&#39;)
    return data_frame


def flatten(data_frame: DataFrame, mode: str = &#39;full&#39;, delimiter: str = &#39;__&#39;) -&gt; DataFrame:
    # cases are full means full explode.
    #           struct only structs,
    #           map will unwind the maps as a pivot and a group by + structs
    #           array will effectively do struct and arrays only (with explode not positional)
    fields: List[StructField] = data_frame.schema.fields
    field_names: list = data_frame.schema.fieldNames()
    for index, value in enumerate(fields):
        field = value
        field_type = field.dataType
        field_name = field.name
        # print(f&#39;{field}, {field_name}, {field_type}, {mode}&#39;)
        if isinstance(field_type, ArrayType) and mode in [&#39;array&#39;, &#39;full&#39;]:
            field_names_excluding_array = [fn for fn in field_names if fn != field_name]
            field_names_to_select = field_names_excluding_array + [
                f&#34;explode_outer({field_name}) as {field_name}&#34;]
            # exploded_df = exploded_df.selectExpr(*field_names_to_select)
            # return flatten0(exploded_df)
            return flatten(data_frame.selectExpr(*field_names_to_select), mode, delimiter)
        elif isinstance(field_type, MapType) and mode in [&#39;map&#39;, &#39;full&#39;]:
            &#34;&#34;&#34;
            This is quite expensive if we do not have a known enumeration of key. 
            Adapted from https://stackoverflow.com/questions/52762487/flattening-maptype-column-in-pyspark
            In this case, we need to create an id column first so that there&#39;s something to group by.
            The pivot here can be expensive, depending on the size of your data.
            posexplode does not work as the pos is associated with a key and value independently.
            so we add an increasing id that we can control
            &#34;&#34;&#34;
            df_left: DataFrame = data_frame \
                .withColumn(&#34;id&#34;, F.monotonically_increasing_id())
            # print(f&#34;{df_left.count()}&#34;)
            # df_left.printSchema()
            df_mapped = df_left \
                .select(&#34;id&#34;, F.explode(field_name)) \
                .withColumn(&#34;key1&#34;, concat(lit(f&#34;{field_name}{delimiter}&#34;), col(&#34;key&#34;))) \
                .drop(&#34;key&#34;) \
                .groupby(&#34;id&#34;) \
                .pivot(&#34;key1&#34;) \
                .agg(F.first(&#39;value&#39;)) \
                .withColumnRenamed(&#34;id&#34;, &#34;id_right&#34;)
            # print(f&#34;{df_mapped.count()}&#34;)
            # df_mapped.printSchema()
            df_flat: DataFrame = df_left.join(df_mapped, df_left.id == df_mapped.id_right, &#34;inner&#34;) \
                .drop(&#34;id&#34;).drop(&#34;id_right&#34;).drop(field_name)
            return flatten(df_flat, mode, delimiter)
        elif isinstance(field_type, StructType):
            child_fieldnames = [f&#34;{field_name}.{child.name}&#34; for child in field_type]
            new_fieldnames = [fn for fn in field_names if fn != field_name] + child_fieldnames
            renamed_cols = [col(x).alias(x.replace(&#34;.&#34;, delimiter)) for x in new_fieldnames]
            # exploded_df = exploded_df.select(*renamed_cols)
            # print(len(exploded_df.schema.fieldNames()))
            # return flatten0(exploded_df)
            return flatten(data_frame.select(*renamed_cols), mode, delimiter)
    # print(f&#39;schema size is {len(data_frame.schema.fieldNames())}&#39;)
    return data_frame


def flatten_p(df: DataFrame, dummy_param_list: list = [None]) -&gt; DataFrame:
    return flatten(df)


# need to make a function @property
def get_random_string(length: int) -&gt; str:
    &#34;&#34;&#34;
    Random string with the combination of lower and upper case
    :param length:
    :return:
    &#34;&#34;&#34;
    letters = ascii_letters
    result_str = &#39;&#39;.join(choice(letters) for _ in range(length))
    return result_str


def add_columns_table(table_name: str, new_columns_mapping_dict: dict) -&gt; DataFrame:
    &#34;&#34;&#34;
    This takes a dict of (new_column: str -&gt; sql_expr: str)
    the approach would be the most efficient as the transform expressions may be quite complex.
    The implication is that UDFs are registered
    :param table_name:
    :param new_columns_mapping_dict: maps of {new_column: str -&gt; sql_expr: str }
    :return:
    &#34;&#34;&#34;
    # note that the table stays with the session and disappears afterwards i we make it a Temp view
    step = new_columns_mapping_dict
    sql_expr = &#39;, &#39;.join([f&#34;{step[c]} as `{c}`&#34; for c in step])
    return spark.sql(f&#39;select *, {sql_expr} from {table_name}&#39;)


def add_columns(df: DataFrame, new_columns_mapping_dict: dict) -&gt; DataFrame:
    &#34;&#34;&#34;
    This takes a dict of (new_column: str -&gt; sql_expr: str)
    the approach would be the most efficient as the transform expressions may be quite complex.
    The implication is that UDFs are registered
    :param df:
    :param new_columns_mapping_dict: maps of {new_column: str -&gt; sql_expr: str }
    :return:
    &#34;&#34;&#34;
    # we can make the table_name a _tmp&lt;randomstring&gt; and drop it at the end...
    # note that the table stays with the session and disappears afterwards i we make it a Temp view
    df.createOrReplaceTempView(&#34;_tmp&#34;)

    return add_columns_table(&#34;_tmp&#34;, new_columns_mapping_dict)


def add_column(df: DataFrame, new_column: str, transform):
    &#34;&#34;&#34;

    :param df:
    :param new_column:
    :param transform:
    :return:
    &#34;&#34;&#34;

    # Assuming all columns are correct... But we better add a check step similar to the drop columns function
    return df.withColumn(new_column, transform)


def genPivotSQL(df: DataFrame, pivoted_columns_list: list = [None]
                , key_column: str = &#39;key_column&#39;
                , value_column: str = &#39;value&#39;
                , fn: str = &#34;max&#34;) -&gt; str:
    pivot_columns = []
    if len(pivoted_columns_list) != 0:
        pivot_columns = pivoted_columns_list
    else:
        pivot_columns = df.select(key_column).distinct().rdd.map(lambda r: r[0]).collect()
        # df.select(&#34;key&#34;).distinct().map(r =&gt; f&#34;{r(0)}&#34;).collect().toList
        # or maybe list comprehension
        # pivot_columns = [i.k for i in df.select(&#39;k&#39;).distinct().collect()]

    # we may need to rewrite the columns to strip/replace characters that are not acceptable for column names
    #  l = s&#34;&#39;${pivotColumns.mkString(&#34;&#39;,&#39;&#34;)}&#39;&#34;
    # rewrite this as a list comprehension?
    # l = pivot_columns.foldLeft(&#34;&#34;)((head, next) =&gt; {s&#34;$head&#39;${next}&#39; ${next.replace(&#34;.&#34;, &#34;__&#34;)},&#34;}).stripSuffix(&#34;,&#34;)
    column_list = &#34;&#39;, &#39;&#34;.join(map(str, pivot_columns))
    df.createOrReplaceTempView(&#34;_tmp&#34;)
    q = f&#34;SELECT * FROM (SELECT * FROM _tmp) PIVOT ( {fn}({value_column}) FOR {key_column} in ( &#39;{column_list}&#39; ))&#34;
    return q


def pivot(df: DataFrame, pivoted_columns_list: list = [None]
          , key_column: str = &#39;key_column&#39;
          , value_column: str = &#39;value&#39;
          , fn: str = &#34;max&#34;) -&gt; DataFrame:
    q = genPivotSQL(df, pivoted_columns_list, key_column, value_column, fn)
    return spark.sql(q)


def pivot_p(df: DataFrame, pivoted_columns_list: list = [None]) -&gt; DataFrame:
    q = genPivotSQL(df, pivoted_columns_list)
    return spark.sql(q)


def filter_op(df: DataFrame, filter_expr_list: list = [&#34;1=1&#34;], operation: str = &#39;AND&#39;) -&gt; DataFrame:
    &#34;&#34;&#34;
    applies a composition of boolean expressions that are either ORed or ANDed together
    :param df:
    :param filter_expr_list: list of filter expressions that return boolean and stitched with the operation (AND or OR)
    :param operation: default to AND
    :return: dataframe filtered down based on the filters
    &#34;&#34;&#34;
    operation_wrapper = f&#34;) {operation} (&#34;
    query_filter = f&#34;({operation_wrapper.join(map(str, filter_expr_list))})&#34;
    return df.filter(query_filter)


def filter_and(df: DataFrame, filter_expr_list: list = [&#34;1=1&#34;]) -&gt; DataFrame:
    &#34;&#34;&#34;
    applies the AND of filter expressions
    :param df:
    :param filter_expr_list: list of filter expressions that return a boolean ANDed together
    :return: dataframe filtered down based on the filters
    &#34;&#34;&#34;
    return filter_op(df, filter_expr_list)


def filter_or(df: DataFrame, filter_expr_list: list = [&#34;1=1&#34;]) -&gt; DataFrame:
    &#34;&#34;&#34;
    applies a union of boolean filter expressions
    :param df:
    :param filter_expr_list: list of filter expressions that return boolean Union-ed together
    :return: dataframe filtered down based on the filters
    &#34;&#34;&#34;
    return filter_op(df, filter_expr_list, &#39;OR&#39;)


def gen_df_column_list(df):
    &#34;&#34;&#34;
    not used for now
    :param df:
    :return:
    &#34;&#34;&#34;
    return df.schema.fieldNames()


def look_up(df: DataFrame, lookup_df: DataFrame, ref_index: int,
            reference_column: str,
            new_column: str,
            lookup_type: str = &#34;value&#34;) -&gt; DataFrame:
    &#34;&#34;&#34;
    # we should have a key for ref instead of index
    :param df:
    :param lookup_df:
    :param ref_index:
    :param reference_column:
    :param new_column:
    :param lookup_type:
    :return:
    &#34;&#34;&#34;

    filtered_lookup_df = lookup_df.filter(f&#34;step={ref_index}&#34;).select(&#34;x&#34;, &#34;y&#34;)
    df_ref = df.join(filtered_lookup_df,
                     col(f&#34;`{reference_column}`&#34;) == filtered_lookup_df.x, &#34;left_outer&#34;)\
        .drop(&#34;x&#34;)
    if lookup_type == &#34;value&#34;:
        return df_ref.withColumnRenamed(&#34;y&#34;, new_column)
    elif lookup_type == &#34;cells&#34;:
        # here we generate the list of columns we would map to column values
        # using toPandas and convert to list... may be faster
        schema_columns = list(df_ref.filter(&#34;y is not null&#34;).select(&#34;y&#34;).distinct().toPandas()[&#39;y&#39;])
        # schema_columns = df_ref.filter(&#34;y is not null&#34;).select(&#34;y&#34;).distinct().rdd.map(lambda r: r[0]).collect()

        # would be nice to really map the value to a col(value) and not use coalesce
        return df_ref.withColumn(new_column, F.coalesce(*[F.when(col(&#34;y&#34;) != f&#34;{c}&#34;, lit(None))
                                                        .otherwise(df_ref[f&#34;`{c}`&#34;]) for c in schema_columns]))\
            .drop(&#34;y&#34;)
        # return df_ref.withColumn(new_column, df_ref[f&#34;`{df_ref.y}`&#34;]).drop(&#34;y&#34;)
        # return df_ref.select(F.coalesce(*[F.when(df_ref.y == c, df_ref[c]).otherwise(None) for c in col_seq]))
    else:
        # nothing to do
        return df


# needs work as this is really 2 dataframes
def add_columns_lookup(df: DataFrame, mapping_dict: dict):
    &#34;&#34;&#34;
    Note todo the lookup file path should be configuration-based?
    otherwise we need it as an independent parameter as it does not make sense to read it for every new column
    as we do not see the case for independent lookup tables
    :param df:
    :param mapping_dict:
    :return:
    &#34;&#34;&#34;
    res_df = df
    for c in mapping_dict:
        params = mapping_dict[c]
        l_df = spark.read.format(&#34;csv&#34;).option(&#34;header&#34;, True).option(&#34;inferSchema&#34;, True).load(params[0])
        ref_index = params[1]
        ref_column = params[2]
        # todo make it more robust
        if len(params) == 3:
            res_df = look_up(res_df, l_df, ref_index, ref_column, c)
        else:
            res_df = look_up(res_df, l_df, ref_index, ref_column, c, params[3])
    return res_df


# this class may be deprecated if we end up having 2 independent code lines (scala and python)
# class DfExtensions(DataFrame):
class DfExtensions:
    &#34;&#34;&#34;
    set of transforms that invoke pystitchr extensions
    wrapper around the scala implementation and interfaces
    &#34;&#34;&#34;

    # from pyspark DataFrame ...
    def __init__(self, jdf, sql_ctx):
        # DataFrame(jdf, sql_ctx)
        self._jdf = jdf
        self.sql_ctx = sql_ctx
        self._sc = sql_ctx and sql_ctx._sc
        self.is_cached = False
        self._schema = None  # initialized lazily
        self._lazy_rdd = None
        # Check whether _repr_html is supported or not, we use it to avoid calling _jdf twice
        # by __repr__ and _repr_html_ while eager evaluation opened.
        self._support_repr_html = False
        # assert isinstance(df, object)
        # DataFrame(df)

    def left_diff_schemas(self, right_df: DataFrame) -&gt; list:
        &#34;&#34;&#34;
        still is done inside pyspark... need to modify to invoke the gateway wrapper
        :param right_df:
        :return:
        &#34;&#34;&#34;
        left_columns_set = set(self.df.schema.names)
        right_columns_set = set(right_df.schema.names)
        # warning: some columns are not in the list... maybe throw a warning error?
        return list(left_columns_set - right_columns_set)

    def right_diff_schemas(self, right_df: DataFrame) -&gt; list:
        &#34;&#34;&#34;
        still is done inside pyspark... need to modify to invoke the gateway wrapper
        :param right_df:
        :return:
        &#34;&#34;&#34;
        left_columns_set = set(self.df.schema.names)
        right_columns_set = set(right_df.schema.names)
        # warning: some columns are not in the list... maybe throw a warning error?
        return list(right_columns_set - left_columns_set)

    def drop_columns(self, drop_columns_list: list):
        &#34;&#34;&#34;

        :param drop_columns_list:
        :return:
        &#34;&#34;&#34;
        df_columns_set = set(self._jdf.schema.names)
        # warning: some columns are not in the list... maybe throw a warning error?
        cols_that_donot_exist = set(drop_columns_list) - df_columns_set
        # get the actual list of columns to drop
        columns2remove = list(set(drop_columns_list) - cols_that_donot_exist)
        # drop and return
        # return self._jdf.drop(*columns2remove)
        # testing 4/9/21
        return DfExtensions(self._jdf.drop(*columns2remove))

    def rename_columns(self, rename_mapping_dict: dict):
        &#34;&#34;&#34;
        :param rename_mapping_dict:
        :return: DataFrame
        Takes a dictionary of columns to be renamed and returns a converted dataframe
        Uses the thin wrapper around spark scala with Py4J
        &#34;&#34;&#34;
        # return DfExtensions(self.sql_ctx._jvm.com.pystitchr.extensions.transform.Dataframe.renameColumns(
        #    _dict_to_scala_map(self._sc, rename_mapping_dict)), self._jdf, self.sql_ctx)
        # return self.sql_ctx._jvm.com.pystitchr.extensions.transform.Dataframe.renameColumns(_dict_to_scala_map(self._sc, rename_mapping_dict), self._jdf)
        # return DfExtensions(self.sql_ctx._jvm.com.pystitchr.extensions.transform.Dataframe.renameColumns(_dict_to_scala_map(self._sc, rename_mapping_dict)), self.sql_ctx)
        return DfExtensions(self._sc._jvm.com.stitchr.extensions.transform.Df.renameColumns(
            _dict_to_scala_map(self._sc, rename_mapping_dict))(self._jdf), self._sc)

    def rename_column(self, existing: str, new: str):
        &#34;&#34;&#34;Returns a new `DataFrame` by renaming an `existing` column to `new` column.
        This is a no-op if the source schema does not contain the given `existing` column name.

        :param existing: string, name of the existing column to rename.
        :param new: string, new name of the column.

        &gt;&gt;&gt; example
        df.withColumnRenamed(&#39;age&#39;, &#39;age2&#39;).collect()
        [Row(age2=2, name=u&#39;Alice&#39;), Row(age2=5, name=u&#39;Bob&#39;)]
        &#34;&#34;&#34;
        # either calls work...
        #
        # return DfExtensions(self._jdf.withColumnRenamed(existing, new), self.sql_ctx)
        return DataFrame(self._jdf.withColumnRenamed(existing, new), self.sql_ctx)


def _dict_to_scala_map(sc, jm):
    &#34;&#34;&#34;
    Convert a dict into a JVM Map.
    &#34;&#34;&#34;
    return sc._jvm.PythonUtils.toScalaMap(jm)


def transform0(self, f):
    &#34;&#34;&#34;
    pyspark does not have a transform before version 3... we need to add one to DataFrame.
    This is based on https://mungingdata.com/pyspark/chaining-dataframe-transformations/
    &#34;&#34;&#34;
    return f(self)


import pystitchr.base.df_transforms as dft
import pystitchr.base.df_functions as fn


def run_pipeline(input_df: DataFrame, pipeline: str)-&gt; DataFrame:
    # don&#39;t want to modify the source so we assign it
    df_p = input_df
    steps = pipeline
    print(f&#34;number of steps is {len(steps)}&#34;)
    for step in steps:
        print(steps[step])
        key = list(steps[step].keys())[0]
        params = steps[step][key]
        print(f&#34;step is {step}, transform is {key} with attributes {params}&#34;)
        method_to_call = getattr(dft, key)
        df_p = df_p.transform(lambda df: method_to_call(df, params))
        df_p.show()
    return df_p


def _test():
    &#34;&#34;&#34;
    test code
    :return:
    &#34;&#34;&#34;
    import os
    from pyspark.sql import SparkSession
    import pyspark.sql.catalog

    os.chdir(os.environ[&#34;SPARK_HOME&#34;])

    globs = pyspark.sql.catalog.__dict__.copy()
    spark = SparkSession.builder \
        .master(&#34;local[4]&#34;) \
        .appName(&#34;sql.catalog tests&#34;) \
        .getOrCreate()
    globs[&#39;sc&#39;] = spark.sparkContext
    globs[&#39;spark&#39;] = spark
    # ...


DataFrame.transform0 = transform0

if __name__ == &#34;__main__&#34;:
    print(&#39;running tests \n&#39;)
    _test()</code></pre>
</details>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="df_transforms.cast_to_spark_type_dict"><code class="name">var <span class="ident">cast_to_spark_type_dict</span></code></dt>
<dd>
<div class="desc"><p>spark = SparkSession.builder.getOrCreate()</p>
<h1 id="import-sparkimplicits_">import spark.implicits._</h1>
<p>spark.sparkContext.setLogLevel('WARN')</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="df_transforms.add_column"><code class="name flex">
<span>def <span class="ident">add_column</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, new_column:Â str, transform)</span>
</code></dt>
<dd>
<div class="desc"><p>:param df:
:param new_column:
:param transform:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_column(df: DataFrame, new_column: str, transform):
    &#34;&#34;&#34;

    :param df:
    :param new_column:
    :param transform:
    :return:
    &#34;&#34;&#34;

    # Assuming all columns are correct... But we better add a check step similar to the drop columns function
    return df.withColumn(new_column, transform)</code></pre>
</details>
</dd>
<dt id="df_transforms.add_columns"><code class="name flex">
<span>def <span class="ident">add_columns</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, new_columns_mapping_dict:Â dict) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>This takes a dict of (new_column: str -&gt; sql_expr: str)
the approach would be the most efficient as the transform expressions may be quite complex.
The implication is that UDFs are registered
:param df:
:param new_columns_mapping_dict: maps of {new_column: str -&gt; sql_expr: str }
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_columns(df: DataFrame, new_columns_mapping_dict: dict) -&gt; DataFrame:
    &#34;&#34;&#34;
    This takes a dict of (new_column: str -&gt; sql_expr: str)
    the approach would be the most efficient as the transform expressions may be quite complex.
    The implication is that UDFs are registered
    :param df:
    :param new_columns_mapping_dict: maps of {new_column: str -&gt; sql_expr: str }
    :return:
    &#34;&#34;&#34;
    # we can make the table_name a _tmp&lt;randomstring&gt; and drop it at the end...
    # note that the table stays with the session and disappears afterwards i we make it a Temp view
    df.createOrReplaceTempView(&#34;_tmp&#34;)

    return add_columns_table(&#34;_tmp&#34;, new_columns_mapping_dict)</code></pre>
</details>
</dd>
<dt id="df_transforms.add_columns_lookup"><code class="name flex">
<span>def <span class="ident">add_columns_lookup</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, mapping_dict:Â dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Note todo the lookup file path should be configuration-based?
otherwise we need it as an independent parameter as it does not make sense to read it for every new column
as we do not see the case for independent lookup tables
:param df:
:param mapping_dict:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_columns_lookup(df: DataFrame, mapping_dict: dict):
    &#34;&#34;&#34;
    Note todo the lookup file path should be configuration-based?
    otherwise we need it as an independent parameter as it does not make sense to read it for every new column
    as we do not see the case for independent lookup tables
    :param df:
    :param mapping_dict:
    :return:
    &#34;&#34;&#34;
    res_df = df
    for c in mapping_dict:
        params = mapping_dict[c]
        l_df = spark.read.format(&#34;csv&#34;).option(&#34;header&#34;, True).option(&#34;inferSchema&#34;, True).load(params[0])
        ref_index = params[1]
        ref_column = params[2]
        # todo make it more robust
        if len(params) == 3:
            res_df = look_up(res_df, l_df, ref_index, ref_column, c)
        else:
            res_df = look_up(res_df, l_df, ref_index, ref_column, c, params[3])
    return res_df</code></pre>
</details>
</dd>
<dt id="df_transforms.add_columns_table"><code class="name flex">
<span>def <span class="ident">add_columns_table</span></span>(<span>table_name:Â str, new_columns_mapping_dict:Â dict) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>This takes a dict of (new_column: str -&gt; sql_expr: str)
the approach would be the most efficient as the transform expressions may be quite complex.
The implication is that UDFs are registered
:param table_name:
:param new_columns_mapping_dict: maps of {new_column: str -&gt; sql_expr: str }
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_columns_table(table_name: str, new_columns_mapping_dict: dict) -&gt; DataFrame:
    &#34;&#34;&#34;
    This takes a dict of (new_column: str -&gt; sql_expr: str)
    the approach would be the most efficient as the transform expressions may be quite complex.
    The implication is that UDFs are registered
    :param table_name:
    :param new_columns_mapping_dict: maps of {new_column: str -&gt; sql_expr: str }
    :return:
    &#34;&#34;&#34;
    # note that the table stays with the session and disappears afterwards i we make it a Temp view
    step = new_columns_mapping_dict
    sql_expr = &#39;, &#39;.join([f&#34;{step[c]} as `{c}`&#34; for c in step])
    return spark.sql(f&#39;select *, {sql_expr} from {table_name}&#39;)</code></pre>
</details>
</dd>
<dt id="df_transforms.drop_columns"><code class="name flex">
<span>def <span class="ident">drop_columns</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, drop_columns_list:Â list) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>:param drop_columns_list:
:param df:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_columns(df: DataFrame, drop_columns_list: list) -&gt; DataFrame:
    &#34;&#34;&#34;

    :param drop_columns_list:
    :param df:
    :return:
    &#34;&#34;&#34;
    df_columns_set = set(df.schema.fieldNames())
    # warn that some columns are not in the list... Or maybe throw an error?
    cols_that_do_not_exist = set(drop_columns_list) - df_columns_set
    # get the actual list of columns to drop
    columns_2_remove = list(set(drop_columns_list) - cols_that_do_not_exist)
    # drop and return
    return df.drop(*columns_2_remove)</code></pre>
</details>
</dd>
<dt id="df_transforms.fields_diff"><code class="name flex">
<span>def <span class="ident">fields_diff</span></span>(<span>left_df:Â pyspark.sql.dataframe.DataFrame, right_df:Â pyspark.sql.dataframe.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fields_diff(left_df: DataFrame, right_df: DataFrame):
    l_set = set(left_df.schema.fieldNames())
    r_set = set(right_df.schema.fieldNames())
    return l_set.difference(r_set), r_set.difference(l_set)</code></pre>
</details>
</dd>
<dt id="df_transforms.filter_and"><code class="name flex">
<span>def <span class="ident">filter_and</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, filter_expr_list:Â listÂ =Â ['1=1']) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>applies the AND of filter expressions
:param df:
:param filter_expr_list: list of filter expressions that return a boolean ANDed together
:return: dataframe filtered down based on the filters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_and(df: DataFrame, filter_expr_list: list = [&#34;1=1&#34;]) -&gt; DataFrame:
    &#34;&#34;&#34;
    applies the AND of filter expressions
    :param df:
    :param filter_expr_list: list of filter expressions that return a boolean ANDed together
    :return: dataframe filtered down based on the filters
    &#34;&#34;&#34;
    return filter_op(df, filter_expr_list)</code></pre>
</details>
</dd>
<dt id="df_transforms.filter_op"><code class="name flex">
<span>def <span class="ident">filter_op</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, filter_expr_list:Â listÂ =Â ['1=1'], operation:Â strÂ =Â 'AND') â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>applies a composition of boolean expressions that are either ORed or ANDed together
:param df:
:param filter_expr_list: list of filter expressions that return boolean and stitched with the operation (AND or OR)
:param operation: default to AND
:return: dataframe filtered down based on the filters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_op(df: DataFrame, filter_expr_list: list = [&#34;1=1&#34;], operation: str = &#39;AND&#39;) -&gt; DataFrame:
    &#34;&#34;&#34;
    applies a composition of boolean expressions that are either ORed or ANDed together
    :param df:
    :param filter_expr_list: list of filter expressions that return boolean and stitched with the operation (AND or OR)
    :param operation: default to AND
    :return: dataframe filtered down based on the filters
    &#34;&#34;&#34;
    operation_wrapper = f&#34;) {operation} (&#34;
    query_filter = f&#34;({operation_wrapper.join(map(str, filter_expr_list))})&#34;
    return df.filter(query_filter)</code></pre>
</details>
</dd>
<dt id="df_transforms.filter_or"><code class="name flex">
<span>def <span class="ident">filter_or</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, filter_expr_list:Â listÂ =Â ['1=1']) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>applies a union of boolean filter expressions
:param df:
:param filter_expr_list: list of filter expressions that return boolean Union-ed together
:return: dataframe filtered down based on the filters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_or(df: DataFrame, filter_expr_list: list = [&#34;1=1&#34;]) -&gt; DataFrame:
    &#34;&#34;&#34;
    applies a union of boolean filter expressions
    :param df:
    :param filter_expr_list: list of filter expressions that return boolean Union-ed together
    :return: dataframe filtered down based on the filters
    &#34;&#34;&#34;
    return filter_op(df, filter_expr_list, &#39;OR&#39;)</code></pre>
</details>
</dd>
<dt id="df_transforms.flatten"><code class="name flex">
<span>def <span class="ident">flatten</span></span>(<span>data_frame:Â pyspark.sql.dataframe.DataFrame, mode:Â strÂ =Â 'full', delimiter:Â strÂ =Â '__') â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatten(data_frame: DataFrame, mode: str = &#39;full&#39;, delimiter: str = &#39;__&#39;) -&gt; DataFrame:
    # cases are full means full explode.
    #           struct only structs,
    #           map will unwind the maps as a pivot and a group by + structs
    #           array will effectively do struct and arrays only (with explode not positional)
    fields: List[StructField] = data_frame.schema.fields
    field_names: list = data_frame.schema.fieldNames()
    for index, value in enumerate(fields):
        field = value
        field_type = field.dataType
        field_name = field.name
        # print(f&#39;{field}, {field_name}, {field_type}, {mode}&#39;)
        if isinstance(field_type, ArrayType) and mode in [&#39;array&#39;, &#39;full&#39;]:
            field_names_excluding_array = [fn for fn in field_names if fn != field_name]
            field_names_to_select = field_names_excluding_array + [
                f&#34;explode_outer({field_name}) as {field_name}&#34;]
            # exploded_df = exploded_df.selectExpr(*field_names_to_select)
            # return flatten0(exploded_df)
            return flatten(data_frame.selectExpr(*field_names_to_select), mode, delimiter)
        elif isinstance(field_type, MapType) and mode in [&#39;map&#39;, &#39;full&#39;]:
            &#34;&#34;&#34;
            This is quite expensive if we do not have a known enumeration of key. 
            Adapted from https://stackoverflow.com/questions/52762487/flattening-maptype-column-in-pyspark
            In this case, we need to create an id column first so that there&#39;s something to group by.
            The pivot here can be expensive, depending on the size of your data.
            posexplode does not work as the pos is associated with a key and value independently.
            so we add an increasing id that we can control
            &#34;&#34;&#34;
            df_left: DataFrame = data_frame \
                .withColumn(&#34;id&#34;, F.monotonically_increasing_id())
            # print(f&#34;{df_left.count()}&#34;)
            # df_left.printSchema()
            df_mapped = df_left \
                .select(&#34;id&#34;, F.explode(field_name)) \
                .withColumn(&#34;key1&#34;, concat(lit(f&#34;{field_name}{delimiter}&#34;), col(&#34;key&#34;))) \
                .drop(&#34;key&#34;) \
                .groupby(&#34;id&#34;) \
                .pivot(&#34;key1&#34;) \
                .agg(F.first(&#39;value&#39;)) \
                .withColumnRenamed(&#34;id&#34;, &#34;id_right&#34;)
            # print(f&#34;{df_mapped.count()}&#34;)
            # df_mapped.printSchema()
            df_flat: DataFrame = df_left.join(df_mapped, df_left.id == df_mapped.id_right, &#34;inner&#34;) \
                .drop(&#34;id&#34;).drop(&#34;id_right&#34;).drop(field_name)
            return flatten(df_flat, mode, delimiter)
        elif isinstance(field_type, StructType):
            child_fieldnames = [f&#34;{field_name}.{child.name}&#34; for child in field_type]
            new_fieldnames = [fn for fn in field_names if fn != field_name] + child_fieldnames
            renamed_cols = [col(x).alias(x.replace(&#34;.&#34;, delimiter)) for x in new_fieldnames]
            # exploded_df = exploded_df.select(*renamed_cols)
            # print(len(exploded_df.schema.fieldNames()))
            # return flatten0(exploded_df)
            return flatten(data_frame.select(*renamed_cols), mode, delimiter)
    # print(f&#39;schema size is {len(data_frame.schema.fieldNames())}&#39;)
    return data_frame</code></pre>
</details>
</dd>
<dt id="df_transforms.flatten0"><code class="name flex">
<span>def <span class="ident">flatten0</span></span>(<span>data_frame:Â pyspark.sql.dataframe.DataFrame) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>NH: Experimental
:param data_frame:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatten0(data_frame: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    NH: Experimental
    :param data_frame:
    :return:
    &#34;&#34;&#34;
    fields: List[StructField] = data_frame.schema.fields
    field_names: list = data_frame.schema.fieldNames()
    # exploded_df = data_frame
    for index, value in enumerate(fields):
        field = value
        field_type = field.dataType
        field_name = field.name
        # print(f&#39;{field}, {field_name}, {field_type}&#39;)
        # we have the case of MapTYpe to handle or isinstance(field_type, MapType)):
        # this means when we see a map we treat as array and add key/value columns?!
        if isinstance(field_type, ArrayType):
            field_names_excluding_array = [fn for fn in field_names if fn != field_name]
            field_names_to_select = field_names_excluding_array + [
                f&#34;explode_outer({field_name}) as {field_name}&#34;]
            # exploded_df = exploded_df.selectExpr(*field_names_to_select)
            # return flatten0(exploded_df)
            return flatten0(data_frame.selectExpr(*field_names_to_select))
        elif isinstance(field_type, MapType):
            &#34;&#34;&#34;
            This is quite expensive if we do not have a known enumeration of key. 
            From https://stackoverflow.com/questions/52762487/flattening-maptype-column-in-pyspark
            df.withColumn(&#34;id&#34;, f.monotonically_increasing_id())\
            .select(&#34;id&#34;, f.explode(&#34;a&#34;))\
            .groupby(&#34;id&#34;)\
            .pivot(&#34;key&#34;)\
            .agg(f.first(&#34;value&#34;))\
            .drop(&#34;id&#34;)\
            In this case, we need to create an id column first so that there&#39;s something to group by.
            The pivot here can be expensive, depending on the size of your data.
            &#34;&#34;&#34;
            &#34;&#34;&#34;
            This solution here outputs a cartesian on each mapped field... 
            Using a pivot like above is better but very expensive
            posexplode does not work so we add an increasing id that we can control
            &#34;&#34;&#34;
            df_mapped = data_frame \
                .withColumn(&#34;id&#34;, F.monotonically_increasing_id()) \
                .select(&#39;*&#39;, F.explode(field_name)) \
                .withColumnRenamed(&#34;key&#34;, f&#34;{field_name}__key_column&#34;) \
                .withColumnRenamed(&#34;value&#34;, f&#34;{field_name}__value&#34;) \
                .withColumnRenamed(&#34;id&#34;, f&#34;{field_name}__id&#34;).drop(field_name)

            return flatten0(df_mapped)
            # return data_frame
        elif isinstance(field_type, StructType):
            child_fieldnames = [f&#34;{field_name}.{child.name}&#34; for child in field_type]
            new_fieldnames = [fn for fn in field_names if fn != field_name] + child_fieldnames
            renamed_cols = [col(x).alias(x.replace(&#34;.&#34;, &#34;__&#34;)) for x in new_fieldnames]
            # exploded_df = exploded_df.select(*renamed_cols)
            # print(len(exploded_df.schema.fieldNames()))
            # return flatten0(exploded_df)
            return flatten0(data_frame.select(*renamed_cols))
    # print(f&#34;schema size is {len(data_frame.schema.fieldNames())}&#34;)
    return data_frame</code></pre>
</details>
</dd>
<dt id="df_transforms.flatten_no_explode"><code class="name flex">
<span>def <span class="ident">flatten_no_explode</span></span>(<span>data_frame:Â pyspark.sql.dataframe.DataFrame) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>NH: experimental &hellip;still under test.... may explode single element arrays (which may also be acceptable)
:param data_frame:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatten_no_explode(data_frame: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    NH: experimental ...still under test.... may explode single element arrays (which may also be acceptable)
    :param data_frame:
    :return:
    &#34;&#34;&#34;
    fields: List[StructField] = data_frame.schema.fields
    field_names: list = data_frame.schema.fieldNames()
    # print(len(field_names))
    # exploded_df = data_frame
    for index, value in enumerate(fields):
        field = value
        field_type = field.dataType
        field_name = field.name
        # print(f&#39;{field}, {field_name}, {field_type}&#39;)
        if isinstance(field_type, StructType):
            child_fieldnames = [f&#34;{field_name}.{child.name}&#34; for child in field_type]
            print(f&#39;{field_name}, {child_fieldnames}&#39;)
            new_fieldnames = [fn for fn in field_names if fn != field_name] + child_fieldnames
            renamed_cols = [col(x).alias(x.replace(&#34;.&#34;, &#34;__&#34;)) for x in new_fieldnames]
            # exploded_df = exploded_df.select(*renamed_cols)
            # print(len(exploded_df.schema.fieldNames()))
            # exploded_df.printSchema()
            # return flatten_no_explode(exploded_df)
            return flatten_no_explode(data_frame.select(*renamed_cols))
    # print(f&#39;schema size is {len(data_frame.schema.fieldNames())}&#39;)
    return data_frame</code></pre>
</details>
</dd>
<dt id="df_transforms.flatten_p"><code class="name flex">
<span>def <span class="ident">flatten_p</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, dummy_param_list:Â listÂ =Â [None]) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatten_p(df: DataFrame, dummy_param_list: list = [None]) -&gt; DataFrame:
    return flatten(df)</code></pre>
</details>
</dd>
<dt id="df_transforms.genPivotSQL"><code class="name flex">
<span>def <span class="ident">genPivotSQL</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, pivoted_columns_list:Â listÂ =Â [None], key_column:Â strÂ =Â 'key_column', value_column:Â strÂ =Â 'value', fn:Â strÂ =Â 'max') â€‘>Â str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def genPivotSQL(df: DataFrame, pivoted_columns_list: list = [None]
                , key_column: str = &#39;key_column&#39;
                , value_column: str = &#39;value&#39;
                , fn: str = &#34;max&#34;) -&gt; str:
    pivot_columns = []
    if len(pivoted_columns_list) != 0:
        pivot_columns = pivoted_columns_list
    else:
        pivot_columns = df.select(key_column).distinct().rdd.map(lambda r: r[0]).collect()
        # df.select(&#34;key&#34;).distinct().map(r =&gt; f&#34;{r(0)}&#34;).collect().toList
        # or maybe list comprehension
        # pivot_columns = [i.k for i in df.select(&#39;k&#39;).distinct().collect()]

    # we may need to rewrite the columns to strip/replace characters that are not acceptable for column names
    #  l = s&#34;&#39;${pivotColumns.mkString(&#34;&#39;,&#39;&#34;)}&#39;&#34;
    # rewrite this as a list comprehension?
    # l = pivot_columns.foldLeft(&#34;&#34;)((head, next) =&gt; {s&#34;$head&#39;${next}&#39; ${next.replace(&#34;.&#34;, &#34;__&#34;)},&#34;}).stripSuffix(&#34;,&#34;)
    column_list = &#34;&#39;, &#39;&#34;.join(map(str, pivot_columns))
    df.createOrReplaceTempView(&#34;_tmp&#34;)
    q = f&#34;SELECT * FROM (SELECT * FROM _tmp) PIVOT ( {fn}({value_column}) FOR {key_column} in ( &#39;{column_list}&#39; ))&#34;
    return q</code></pre>
</details>
</dd>
<dt id="df_transforms.gen_df_column_list"><code class="name flex">
<span>def <span class="ident">gen_df_column_list</span></span>(<span>df)</span>
</code></dt>
<dd>
<div class="desc"><p>not used for now
:param df:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gen_df_column_list(df):
    &#34;&#34;&#34;
    not used for now
    :param df:
    :return:
    &#34;&#34;&#34;
    return df.schema.fieldNames()</code></pre>
</details>
</dd>
<dt id="df_transforms.generate_missing_columns"><code class="name flex">
<span>def <span class="ident">generate_missing_columns</span></span>(<span>domain:Â str, columns:Â list, attributes_df:Â pyspark.sql.dataframe.DataFrame) â€‘>Â list</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_missing_columns(domain: str, columns: list, attributes_df: DataFrame) -&gt; list:
    # using call by string name getattr()
    col_df = spark.createDataFrame(columns, StringType())
    column_meta_df = attributes_df \
        .filter(f&#34;domain_prefix = &#39;{domain}&#39;&#34;) \
        .select(&#34;sequence&#34;, &#34;variable_name&#34;, &#34;datatype&#34;, &#34;core&#34;)
    # NH need to test that the order is conserved...
    meta_df = col_df.join(column_meta_df, col_df.value == column_meta_df.variable_name, &#34;right&#34;) \
        .filter(&#34;value is null&#34;).orderBy(&#34;sequence&#34;)

    # meta_df.show(50, False)
    # meta_df.printSchema()
    column_meta = meta_df.collect()
    print(column_meta)
    m = list(map(lambda column: (column.sequence, column.variable_name, cast_to_spark_type_dict.get(column.datatype)),
                 column_meta))
    return m</code></pre>
</details>
</dd>
<dt id="df_transforms.generate_schema_by_string"><code class="name flex">
<span>def <span class="ident">generate_schema_by_string</span></span>(<span>domain:Â str, columns:Â list, attributes_df:Â pyspark.sql.dataframe.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_schema_by_string(domain: str, columns: list, attributes_df: DataFrame):
    import pyspark.sql.types as t
    filter_string = &#34;&#39;,&#39;&#34;.join(columns)
    col_df = spark.createDataFrame(columns, StringType())

    column_meta_df = attributes_df \
        .filter(f&#34;domain_prefix = &#39;{domain}&#39;&#34;) \
        .select(&#34;sequence&#34;, &#34;variable_name&#34;, &#34;datatype&#34;, &#34;core&#34;)
    # NH need to test that the order is conserved...
    meta_df = col_df.join(column_meta_df, col_df.value == column_meta_df.variable_name, &#34;left&#34;) \
        .drop(&#34;variable_name&#34;) \
        .withColumnRenamed(&#34;value&#34;, &#34;variable_name&#34;) \
        .withColumn(&#34;datatype&#34;, when(column_meta_df.datatype.isNull(), &#34;Unknown&#34;)
                    .otherwise(column_meta_df.datatype))

    # meta_df.show(50, False)
    # meta_df.printSchema()
    column_meta = meta_df.collect()
    print(column_meta)
    m = list(map(lambda column: StructField(column.variable_name,
                                            getattr(t, to_spark_type_dict_by_string[column.datatype])(), True),
                 column_meta))
    return StructType(m)</code></pre>
</details>
</dd>
<dt id="df_transforms.get_random_string"><code class="name flex">
<span>def <span class="ident">get_random_string</span></span>(<span>length:Â int) â€‘>Â str</span>
</code></dt>
<dd>
<div class="desc"><p>Random string with the combination of lower and upper case
:param length:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_random_string(length: int) -&gt; str:
    &#34;&#34;&#34;
    Random string with the combination of lower and upper case
    :param length:
    :return:
    &#34;&#34;&#34;
    letters = ascii_letters
    result_str = &#39;&#39;.join(choice(letters) for _ in range(length))
    return result_str</code></pre>
</details>
</dd>
<dt id="df_transforms.left_diff_schemas"><code class="name flex">
<span>def <span class="ident">left_diff_schemas</span></span>(<span>left_df:Â pyspark.sql.dataframe.DataFrame, right_df:Â pyspark.sql.dataframe.DataFrame) â€‘>Â list</span>
</code></dt>
<dd>
<div class="desc"><p>:param left_df:
:param right_df:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def left_diff_schemas(left_df: DataFrame, right_df: DataFrame) -&gt; list:
    &#34;&#34;&#34;

    :param left_df:
    :param right_df:
    :return:
    &#34;&#34;&#34;
    left_columns_set = set(left_df.schema.fieldNames())
    right_columns_set = set(right_df.schema.fieldNames())
    # what is the toSet on python? .toSet
    # print(list(df_columns_set))
    # warn that some columns are not in the list... Or maybe throw an error?
    return list(left_columns_set - right_columns_set)</code></pre>
</details>
</dd>
<dt id="df_transforms.look_up"><code class="name flex">
<span>def <span class="ident">look_up</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, lookup_df:Â pyspark.sql.dataframe.DataFrame, ref_index:Â int, reference_column:Â str, new_column:Â str, lookup_type:Â strÂ =Â 'value') â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><h1 id="we-should-have-a-key-for-ref-instead-of-index">we should have a key for ref instead of index</h1>
<p>:param df:
:param lookup_df:
:param ref_index:
:param reference_column:
:param new_column:
:param lookup_type:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def look_up(df: DataFrame, lookup_df: DataFrame, ref_index: int,
            reference_column: str,
            new_column: str,
            lookup_type: str = &#34;value&#34;) -&gt; DataFrame:
    &#34;&#34;&#34;
    # we should have a key for ref instead of index
    :param df:
    :param lookup_df:
    :param ref_index:
    :param reference_column:
    :param new_column:
    :param lookup_type:
    :return:
    &#34;&#34;&#34;

    filtered_lookup_df = lookup_df.filter(f&#34;step={ref_index}&#34;).select(&#34;x&#34;, &#34;y&#34;)
    df_ref = df.join(filtered_lookup_df,
                     col(f&#34;`{reference_column}`&#34;) == filtered_lookup_df.x, &#34;left_outer&#34;)\
        .drop(&#34;x&#34;)
    if lookup_type == &#34;value&#34;:
        return df_ref.withColumnRenamed(&#34;y&#34;, new_column)
    elif lookup_type == &#34;cells&#34;:
        # here we generate the list of columns we would map to column values
        # using toPandas and convert to list... may be faster
        schema_columns = list(df_ref.filter(&#34;y is not null&#34;).select(&#34;y&#34;).distinct().toPandas()[&#39;y&#39;])
        # schema_columns = df_ref.filter(&#34;y is not null&#34;).select(&#34;y&#34;).distinct().rdd.map(lambda r: r[0]).collect()

        # would be nice to really map the value to a col(value) and not use coalesce
        return df_ref.withColumn(new_column, F.coalesce(*[F.when(col(&#34;y&#34;) != f&#34;{c}&#34;, lit(None))
                                                        .otherwise(df_ref[f&#34;`{c}`&#34;]) for c in schema_columns]))\
            .drop(&#34;y&#34;)
        # return df_ref.withColumn(new_column, df_ref[f&#34;`{df_ref.y}`&#34;]).drop(&#34;y&#34;)
        # return df_ref.select(F.coalesce(*[F.when(df_ref.y == c, df_ref[c]).otherwise(None) for c in col_seq]))
    else:
        # nothing to do
        return df</code></pre>
</details>
</dd>
<dt id="df_transforms.pivot"><code class="name flex">
<span>def <span class="ident">pivot</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, pivoted_columns_list:Â listÂ =Â [None], key_column:Â strÂ =Â 'key_column', value_column:Â strÂ =Â 'value', fn:Â strÂ =Â 'max') â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pivot(df: DataFrame, pivoted_columns_list: list = [None]
          , key_column: str = &#39;key_column&#39;
          , value_column: str = &#39;value&#39;
          , fn: str = &#34;max&#34;) -&gt; DataFrame:
    q = genPivotSQL(df, pivoted_columns_list, key_column, value_column, fn)
    return spark.sql(q)</code></pre>
</details>
</dd>
<dt id="df_transforms.pivot_p"><code class="name flex">
<span>def <span class="ident">pivot_p</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, pivoted_columns_list:Â listÂ =Â [None]) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pivot_p(df: DataFrame, pivoted_columns_list: list = [None]) -&gt; DataFrame:
    q = genPivotSQL(df, pivoted_columns_list)
    return spark.sql(q)</code></pre>
</details>
</dd>
<dt id="df_transforms.rename_4_parquet"><code class="name flex">
<span>def <span class="ident">rename_4_parquet</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>rename all columns of the dataFrame so that we can save as a Parquet file
:param df:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rename_4_parquet(df: DataFrame) -&gt; DataFrame:
    &#34;&#34;&#34;
    rename all columns of the dataFrame so that we can save as a Parquet file
    :param df:
    :return:
    &#34;&#34;&#34;
    # need to add left/right trims and replace multiple __ with one?
    # r = &#34;[ ,;{}()\n\t=]&#34;
    # added &#34;.&#34; and &#34;-&#34; and / so that we skip using ``
    # regex = r&#34;[ ,;{}()\n\t=.-]&#34;
    regex = r&#34;[- ,;{}()\n\t=./]&#34;
    delimiter = &#39;__&#39;
    schema_fields = df.schema.fields
    return spark.createDataFrame(
        df.rdd,
        StructType(
            # [StructField(re.sub(regex, delimiter, sf.name.replace(&#39; &#39;, &#39;__&#39;)), sf.dataType, sf.nullable) for sf in
            [StructField(re.sub(regex, delimiter, sf.name.replace(&#39; &#39;, &#39;&#39;)), sf.dataType, sf.nullable) for sf in
             schema_fields]
        )
    )</code></pre>
</details>
</dd>
<dt id="df_transforms.rename_4_parquet_p"><code class="name flex">
<span>def <span class="ident">rename_4_parquet_p</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, dummy_list:Â listÂ =Â [None]) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rename_4_parquet_p(df: DataFrame, dummy_list: list = [None]) -&gt; DataFrame:
    return rename_4_parquet(df)</code></pre>
</details>
</dd>
<dt id="df_transforms.rename_column"><code class="name flex">
<span>def <span class="ident">rename_column</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, existing:Â str, new:Â str) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a new :class:<code>DataFrame</code> by renaming an existing column.
This is a no-op if schema doesn't contain the given column name.
Effectively a wrapper over withColumnRenamed</p>
<p>:param df:
:rtype: object
:param existing: string, name of the existing column to rename.
:param new: string, new name of the column.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; df.rename_column('age', 'age2').collect()
[Row(age2=2, name=u'Alice'), Row(age2=5, name=u'Bob')]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rename_column(df: DataFrame, existing: str, new: str) -&gt; DataFrame:
    &#34;&#34;&#34;Returns a new :class:`DataFrame` by renaming an existing column.
    This is a no-op if schema doesn&#39;t contain the given column name.
    Effectively a wrapper over withColumnRenamed

    :param df:
    :rtype: object
    :param existing: string, name of the existing column to rename.
    :param new: string, new name of the column.

    &gt;&gt;&gt; df.rename_column(&#39;age&#39;, &#39;age2&#39;).collect()
    [Row(age2=2, name=u&#39;Alice&#39;), Row(age2=5, name=u&#39;Bob&#39;)]
    &#34;&#34;&#34;
    return df.withColumnRenamed(existing, new)</code></pre>
</details>
</dd>
<dt id="df_transforms.rename_columns"><code class="name flex">
<span>def <span class="ident">rename_columns</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, rename_mapping_dict:Â dict) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Caveat: NH: need to fix. The outcome changes the type to string
:param df:
:param rename_mapping_dict:
:return: DataFrame
Takes a dictionary of columns to be renamed and returns a converted dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rename_columns(df: DataFrame, rename_mapping_dict: dict) -&gt; DataFrame:
    &#34;&#34;&#34;
    Caveat: NH: need to fix. The outcome changes the type to string
    :param df:
    :param rename_mapping_dict:
    :return: DataFrame
    Takes a dictionary of columns to be renamed and returns a converted dataframe
    &#34;&#34;&#34;
    # we use iteration over the dict and call df.WithColumn
    # this is not efficient?
    df_columns: list = df.schema.fieldNames()
    df_new_columns: list = [rename_mapping_dict[c] if (c in rename_mapping_dict)
                            else c
                            for c in df_columns]
    # df_new_columns: list = [f&#34;`{rename_mapping_dict[c]}`&#34; if (c in rename_mapping_dict)
    #                         else c
    #                         for c in df_columns]
    return df.toDF(*df_new_columns)</code></pre>
</details>
</dd>
<dt id="df_transforms.right_diff_schemas"><code class="name flex">
<span>def <span class="ident">right_diff_schemas</span></span>(<span>left_df:Â pyspark.sql.dataframe.DataFrame, right_df:Â pyspark.sql.dataframe.DataFrame) â€‘>Â list</span>
</code></dt>
<dd>
<div class="desc"><p>:param left_df:
:param right_df:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def right_diff_schemas(left_df: DataFrame, right_df: DataFrame) -&gt; list:
    &#34;&#34;&#34;

    :param left_df:
    :param right_df:
    :return:
    &#34;&#34;&#34;
    left_columns_set = set(left_df.schema.names)
    right_columns_set = set(right_df.schema.names)
    # what is the toSet on python? .toSet
    # print(list(df_columns_set))
    # warn that some columns are not in the list... Or maybe throw an error?
    return list(right_columns_set - left_columns_set)</code></pre>
</details>
</dd>
<dt id="df_transforms.run_pipeline"><code class="name flex">
<span>def <span class="ident">run_pipeline</span></span>(<span>input_df:Â pyspark.sql.dataframe.DataFrame, pipeline:Â str) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_pipeline(input_df: DataFrame, pipeline: str)-&gt; DataFrame:
    # don&#39;t want to modify the source so we assign it
    df_p = input_df
    steps = pipeline
    print(f&#34;number of steps is {len(steps)}&#34;)
    for step in steps:
        print(steps[step])
        key = list(steps[step].keys())[0]
        params = steps[step][key]
        print(f&#34;step is {step}, transform is {key} with attributes {params}&#34;)
        method_to_call = getattr(dft, key)
        df_p = df_p.transform(lambda df: method_to_call(df, params))
        df_p.show()
    return df_p</code></pre>
</details>
</dd>
<dt id="df_transforms.schema_diff"><code class="name flex">
<span>def <span class="ident">schema_diff</span></span>(<span>left_df:Â pyspark.sql.dataframe.DataFrame, right_df:Â pyspark.sql.dataframe.DataFrame)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def schema_diff(left_df: DataFrame, right_df: DataFrame):
    right_columns_set = set(right_df.schema)
    left_columns_set = set(left_df.schema)
    return ([l for l in left_df.schema if l not in right_columns_set],
            [r for r in right_df.schema if r not in left_columns_set]
            )</code></pre>
</details>
</dd>
<dt id="df_transforms.select_exclude"><code class="name flex">
<span>def <span class="ident">select_exclude</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, columns_2_exclude:Â list) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>:param df:
:param columns_2_exclude:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_exclude(df: DataFrame, columns_2_exclude: list) -&gt; DataFrame:
    &#34;&#34;&#34;
    :param df:
    :param columns_2_exclude:
    :return:
    &#34;&#34;&#34;
    column_list: list = list(set(df.schema.fieldNames()) - set(columns_2_exclude))
    return df.select(*column_list)</code></pre>
</details>
</dd>
<dt id="df_transforms.select_list"><code class="name flex">
<span>def <span class="ident">select_list</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, column_list:Â list) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>:param df:
:param column_list:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_list(df: DataFrame, column_list: list) -&gt; DataFrame:
    &#34;&#34;&#34;

    :param df:
    :param column_list:
    :return:
    &#34;&#34;&#34;
    cl: list = f&#34;`{&#39;`,`&#39;.join(column_list)}`&#34;.split(&#39;,&#39;)
    return df.select(*cl)</code></pre>
</details>
</dd>
<dt id="df_transforms.transform0"><code class="name flex">
<span>def <span class="ident">transform0</span></span>(<span>self, f)</span>
</code></dt>
<dd>
<div class="desc"><p>pyspark does not have a transform before version 3&hellip; we need to add one to DataFrame.
This is based on <a href="https://mungingdata.com/pyspark/chaining-dataframe-transformations/">https://mungingdata.com/pyspark/chaining-dataframe-transformations/</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform0(self, f):
    &#34;&#34;&#34;
    pyspark does not have a transform before version 3... we need to add one to DataFrame.
    This is based on https://mungingdata.com/pyspark/chaining-dataframe-transformations/
    &#34;&#34;&#34;
    return f(self)</code></pre>
</details>
</dd>
<dt id="df_transforms.unpivot"><code class="name flex">
<span>def <span class="ident">unpivot</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, unpivot_keys:Â list, unpivot_column_list:Â list, key_column:Â strÂ =Â 'key_column', value_column:Â strÂ =Â 'value') â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>:param df:
:param unpivot_keys:
:param unpivot_column_list:
:param key_column:
:param value_column:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unpivot(df: DataFrame, unpivot_keys: list,
            unpivot_column_list: list,
            key_column: str = &#34;key_column&#34;,
            value_column: str = &#34;value&#34;) -&gt; DataFrame:
    &#34;&#34;&#34;

    :param df:
    :param unpivot_keys:
    :param unpivot_column_list:
    :param key_column:
    :param value_column:
    :return:
    &#34;&#34;&#34;
    # we can improve by checking the parameter lists to be in the schema
    stack_fields_array = unpivot_column_list
    # we need to cast to STRING as we may have int, double , etc... we would couple this with extracting the types
    pivot_map_list = [f&#34;&#39;{s.replace(&#39;`&#39;, &#39;&#39;)}&#39;, STRING(`{s}`) &#34; for s in stack_fields_array]
    stack_fields: str = f&#34;stack({len(stack_fields_array)},{&#39;,&#39;.join([str(x) for x in pivot_map_list])})&#34;
    df.createOrReplaceTempView(&#39;_unpivot&#39;)
    q = f&#34;select `{&#39;`,`&#39;.join([str(x) for x in unpivot_keys])}`, {stack_fields} as (`{key_column}`, `{value_column}`) &#34; \
        f&#34;from _unpivot&#34;
    # replace with logging ... print(f&#39;&#39;&#39;query is: {q} \n&#39;&#39;&#39;)
    return spark.sql(q)</code></pre>
</details>
</dd>
<dt id="df_transforms.unpivot_p"><code class="name flex">
<span>def <span class="ident">unpivot_p</span></span>(<span>df:Â pyspark.sql.dataframe.DataFrame, params:Â list) â€‘>Â pyspark.sql.dataframe.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unpivot_p(df: DataFrame, params: list) -&gt; DataFrame:
    _keys = params[0]
    _unpivot_list = params[1]
    if len(_keys) == 0:
        _keys = list(set(df.schema.fieldNames()).difference(set(_unpivot_list)))
    return unpivot(df, _keys, _unpivot_list)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="df_transforms.DfExtensions"><code class="flex name class">
<span>class <span class="ident">DfExtensions</span></span>
<span>(</span><span>jdf, sql_ctx)</span>
</code></dt>
<dd>
<div class="desc"><p>set of transforms that invoke pystitchr extensions
wrapper around the scala implementation and interfaces</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DfExtensions:
    &#34;&#34;&#34;
    set of transforms that invoke pystitchr extensions
    wrapper around the scala implementation and interfaces
    &#34;&#34;&#34;

    # from pyspark DataFrame ...
    def __init__(self, jdf, sql_ctx):
        # DataFrame(jdf, sql_ctx)
        self._jdf = jdf
        self.sql_ctx = sql_ctx
        self._sc = sql_ctx and sql_ctx._sc
        self.is_cached = False
        self._schema = None  # initialized lazily
        self._lazy_rdd = None
        # Check whether _repr_html is supported or not, we use it to avoid calling _jdf twice
        # by __repr__ and _repr_html_ while eager evaluation opened.
        self._support_repr_html = False
        # assert isinstance(df, object)
        # DataFrame(df)

    def left_diff_schemas(self, right_df: DataFrame) -&gt; list:
        &#34;&#34;&#34;
        still is done inside pyspark... need to modify to invoke the gateway wrapper
        :param right_df:
        :return:
        &#34;&#34;&#34;
        left_columns_set = set(self.df.schema.names)
        right_columns_set = set(right_df.schema.names)
        # warning: some columns are not in the list... maybe throw a warning error?
        return list(left_columns_set - right_columns_set)

    def right_diff_schemas(self, right_df: DataFrame) -&gt; list:
        &#34;&#34;&#34;
        still is done inside pyspark... need to modify to invoke the gateway wrapper
        :param right_df:
        :return:
        &#34;&#34;&#34;
        left_columns_set = set(self.df.schema.names)
        right_columns_set = set(right_df.schema.names)
        # warning: some columns are not in the list... maybe throw a warning error?
        return list(right_columns_set - left_columns_set)

    def drop_columns(self, drop_columns_list: list):
        &#34;&#34;&#34;

        :param drop_columns_list:
        :return:
        &#34;&#34;&#34;
        df_columns_set = set(self._jdf.schema.names)
        # warning: some columns are not in the list... maybe throw a warning error?
        cols_that_donot_exist = set(drop_columns_list) - df_columns_set
        # get the actual list of columns to drop
        columns2remove = list(set(drop_columns_list) - cols_that_donot_exist)
        # drop and return
        # return self._jdf.drop(*columns2remove)
        # testing 4/9/21
        return DfExtensions(self._jdf.drop(*columns2remove))

    def rename_columns(self, rename_mapping_dict: dict):
        &#34;&#34;&#34;
        :param rename_mapping_dict:
        :return: DataFrame
        Takes a dictionary of columns to be renamed and returns a converted dataframe
        Uses the thin wrapper around spark scala with Py4J
        &#34;&#34;&#34;
        # return DfExtensions(self.sql_ctx._jvm.com.pystitchr.extensions.transform.Dataframe.renameColumns(
        #    _dict_to_scala_map(self._sc, rename_mapping_dict)), self._jdf, self.sql_ctx)
        # return self.sql_ctx._jvm.com.pystitchr.extensions.transform.Dataframe.renameColumns(_dict_to_scala_map(self._sc, rename_mapping_dict), self._jdf)
        # return DfExtensions(self.sql_ctx._jvm.com.pystitchr.extensions.transform.Dataframe.renameColumns(_dict_to_scala_map(self._sc, rename_mapping_dict)), self.sql_ctx)
        return DfExtensions(self._sc._jvm.com.stitchr.extensions.transform.Df.renameColumns(
            _dict_to_scala_map(self._sc, rename_mapping_dict))(self._jdf), self._sc)

    def rename_column(self, existing: str, new: str):
        &#34;&#34;&#34;Returns a new `DataFrame` by renaming an `existing` column to `new` column.
        This is a no-op if the source schema does not contain the given `existing` column name.

        :param existing: string, name of the existing column to rename.
        :param new: string, new name of the column.

        &gt;&gt;&gt; example
        df.withColumnRenamed(&#39;age&#39;, &#39;age2&#39;).collect()
        [Row(age2=2, name=u&#39;Alice&#39;), Row(age2=5, name=u&#39;Bob&#39;)]
        &#34;&#34;&#34;
        # either calls work...
        #
        # return DfExtensions(self._jdf.withColumnRenamed(existing, new), self.sql_ctx)
        return DataFrame(self._jdf.withColumnRenamed(existing, new), self.sql_ctx)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="df_transforms.DfExtensions.drop_columns"><code class="name flex">
<span>def <span class="ident">drop_columns</span></span>(<span>self, drop_columns_list:Â list)</span>
</code></dt>
<dd>
<div class="desc"><p>:param drop_columns_list:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_columns(self, drop_columns_list: list):
    &#34;&#34;&#34;

    :param drop_columns_list:
    :return:
    &#34;&#34;&#34;
    df_columns_set = set(self._jdf.schema.names)
    # warning: some columns are not in the list... maybe throw a warning error?
    cols_that_donot_exist = set(drop_columns_list) - df_columns_set
    # get the actual list of columns to drop
    columns2remove = list(set(drop_columns_list) - cols_that_donot_exist)
    # drop and return
    # return self._jdf.drop(*columns2remove)
    # testing 4/9/21
    return DfExtensions(self._jdf.drop(*columns2remove))</code></pre>
</details>
</dd>
<dt id="df_transforms.DfExtensions.left_diff_schemas"><code class="name flex">
<span>def <span class="ident">left_diff_schemas</span></span>(<span>self, right_df:Â pyspark.sql.dataframe.DataFrame) â€‘>Â list</span>
</code></dt>
<dd>
<div class="desc"><p>still is done inside pyspark&hellip; need to modify to invoke the gateway wrapper
:param right_df:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def left_diff_schemas(self, right_df: DataFrame) -&gt; list:
    &#34;&#34;&#34;
    still is done inside pyspark... need to modify to invoke the gateway wrapper
    :param right_df:
    :return:
    &#34;&#34;&#34;
    left_columns_set = set(self.df.schema.names)
    right_columns_set = set(right_df.schema.names)
    # warning: some columns are not in the list... maybe throw a warning error?
    return list(left_columns_set - right_columns_set)</code></pre>
</details>
</dd>
<dt id="df_transforms.DfExtensions.rename_column"><code class="name flex">
<span>def <span class="ident">rename_column</span></span>(<span>self, existing:Â str, new:Â str)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a new <code>DataFrame</code> by renaming an <code>existing</code> column to <code>new</code> column.
This is a no-op if the source schema does not contain the given <code>existing</code> column name.</p>
<p>:param existing: string, name of the existing column to rename.
:param new: string, new name of the column.</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; example
df.withColumnRenamed('age', 'age2').collect()
[Row(age2=2, name=u'Alice'), Row(age2=5, name=u'Bob')]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rename_column(self, existing: str, new: str):
    &#34;&#34;&#34;Returns a new `DataFrame` by renaming an `existing` column to `new` column.
    This is a no-op if the source schema does not contain the given `existing` column name.

    :param existing: string, name of the existing column to rename.
    :param new: string, new name of the column.

    &gt;&gt;&gt; example
    df.withColumnRenamed(&#39;age&#39;, &#39;age2&#39;).collect()
    [Row(age2=2, name=u&#39;Alice&#39;), Row(age2=5, name=u&#39;Bob&#39;)]
    &#34;&#34;&#34;
    # either calls work...
    #
    # return DfExtensions(self._jdf.withColumnRenamed(existing, new), self.sql_ctx)
    return DataFrame(self._jdf.withColumnRenamed(existing, new), self.sql_ctx)</code></pre>
</details>
</dd>
<dt id="df_transforms.DfExtensions.rename_columns"><code class="name flex">
<span>def <span class="ident">rename_columns</span></span>(<span>self, rename_mapping_dict:Â dict)</span>
</code></dt>
<dd>
<div class="desc"><p>:param rename_mapping_dict:
:return: DataFrame
Takes a dictionary of columns to be renamed and returns a converted dataframe
Uses the thin wrapper around spark scala with Py4J</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rename_columns(self, rename_mapping_dict: dict):
    &#34;&#34;&#34;
    :param rename_mapping_dict:
    :return: DataFrame
    Takes a dictionary of columns to be renamed and returns a converted dataframe
    Uses the thin wrapper around spark scala with Py4J
    &#34;&#34;&#34;
    # return DfExtensions(self.sql_ctx._jvm.com.pystitchr.extensions.transform.Dataframe.renameColumns(
    #    _dict_to_scala_map(self._sc, rename_mapping_dict)), self._jdf, self.sql_ctx)
    # return self.sql_ctx._jvm.com.pystitchr.extensions.transform.Dataframe.renameColumns(_dict_to_scala_map(self._sc, rename_mapping_dict), self._jdf)
    # return DfExtensions(self.sql_ctx._jvm.com.pystitchr.extensions.transform.Dataframe.renameColumns(_dict_to_scala_map(self._sc, rename_mapping_dict)), self.sql_ctx)
    return DfExtensions(self._sc._jvm.com.stitchr.extensions.transform.Df.renameColumns(
        _dict_to_scala_map(self._sc, rename_mapping_dict))(self._jdf), self._sc)</code></pre>
</details>
</dd>
<dt id="df_transforms.DfExtensions.right_diff_schemas"><code class="name flex">
<span>def <span class="ident">right_diff_schemas</span></span>(<span>self, right_df:Â pyspark.sql.dataframe.DataFrame) â€‘>Â list</span>
</code></dt>
<dd>
<div class="desc"><p>still is done inside pyspark&hellip; need to modify to invoke the gateway wrapper
:param right_df:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def right_diff_schemas(self, right_df: DataFrame) -&gt; list:
    &#34;&#34;&#34;
    still is done inside pyspark... need to modify to invoke the gateway wrapper
    :param right_df:
    :return:
    &#34;&#34;&#34;
    left_columns_set = set(self.df.schema.names)
    right_columns_set = set(right_df.schema.names)
    # warning: some columns are not in the list... maybe throw a warning error?
    return list(right_columns_set - left_columns_set)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="df_transforms.cast_to_spark_type_dict" href="#df_transforms.cast_to_spark_type_dict">cast_to_spark_type_dict</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="df_transforms.add_column" href="#df_transforms.add_column">add_column</a></code></li>
<li><code><a title="df_transforms.add_columns" href="#df_transforms.add_columns">add_columns</a></code></li>
<li><code><a title="df_transforms.add_columns_lookup" href="#df_transforms.add_columns_lookup">add_columns_lookup</a></code></li>
<li><code><a title="df_transforms.add_columns_table" href="#df_transforms.add_columns_table">add_columns_table</a></code></li>
<li><code><a title="df_transforms.drop_columns" href="#df_transforms.drop_columns">drop_columns</a></code></li>
<li><code><a title="df_transforms.fields_diff" href="#df_transforms.fields_diff">fields_diff</a></code></li>
<li><code><a title="df_transforms.filter_and" href="#df_transforms.filter_and">filter_and</a></code></li>
<li><code><a title="df_transforms.filter_op" href="#df_transforms.filter_op">filter_op</a></code></li>
<li><code><a title="df_transforms.filter_or" href="#df_transforms.filter_or">filter_or</a></code></li>
<li><code><a title="df_transforms.flatten" href="#df_transforms.flatten">flatten</a></code></li>
<li><code><a title="df_transforms.flatten0" href="#df_transforms.flatten0">flatten0</a></code></li>
<li><code><a title="df_transforms.flatten_no_explode" href="#df_transforms.flatten_no_explode">flatten_no_explode</a></code></li>
<li><code><a title="df_transforms.flatten_p" href="#df_transforms.flatten_p">flatten_p</a></code></li>
<li><code><a title="df_transforms.genPivotSQL" href="#df_transforms.genPivotSQL">genPivotSQL</a></code></li>
<li><code><a title="df_transforms.gen_df_column_list" href="#df_transforms.gen_df_column_list">gen_df_column_list</a></code></li>
<li><code><a title="df_transforms.generate_missing_columns" href="#df_transforms.generate_missing_columns">generate_missing_columns</a></code></li>
<li><code><a title="df_transforms.generate_schema_by_string" href="#df_transforms.generate_schema_by_string">generate_schema_by_string</a></code></li>
<li><code><a title="df_transforms.get_random_string" href="#df_transforms.get_random_string">get_random_string</a></code></li>
<li><code><a title="df_transforms.left_diff_schemas" href="#df_transforms.left_diff_schemas">left_diff_schemas</a></code></li>
<li><code><a title="df_transforms.look_up" href="#df_transforms.look_up">look_up</a></code></li>
<li><code><a title="df_transforms.pivot" href="#df_transforms.pivot">pivot</a></code></li>
<li><code><a title="df_transforms.pivot_p" href="#df_transforms.pivot_p">pivot_p</a></code></li>
<li><code><a title="df_transforms.rename_4_parquet" href="#df_transforms.rename_4_parquet">rename_4_parquet</a></code></li>
<li><code><a title="df_transforms.rename_4_parquet_p" href="#df_transforms.rename_4_parquet_p">rename_4_parquet_p</a></code></li>
<li><code><a title="df_transforms.rename_column" href="#df_transforms.rename_column">rename_column</a></code></li>
<li><code><a title="df_transforms.rename_columns" href="#df_transforms.rename_columns">rename_columns</a></code></li>
<li><code><a title="df_transforms.right_diff_schemas" href="#df_transforms.right_diff_schemas">right_diff_schemas</a></code></li>
<li><code><a title="df_transforms.run_pipeline" href="#df_transforms.run_pipeline">run_pipeline</a></code></li>
<li><code><a title="df_transforms.schema_diff" href="#df_transforms.schema_diff">schema_diff</a></code></li>
<li><code><a title="df_transforms.select_exclude" href="#df_transforms.select_exclude">select_exclude</a></code></li>
<li><code><a title="df_transforms.select_list" href="#df_transforms.select_list">select_list</a></code></li>
<li><code><a title="df_transforms.transform0" href="#df_transforms.transform0">transform0</a></code></li>
<li><code><a title="df_transforms.unpivot" href="#df_transforms.unpivot">unpivot</a></code></li>
<li><code><a title="df_transforms.unpivot_p" href="#df_transforms.unpivot_p">unpivot_p</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="df_transforms.DfExtensions" href="#df_transforms.DfExtensions">DfExtensions</a></code></h4>
<ul class="">
<li><code><a title="df_transforms.DfExtensions.drop_columns" href="#df_transforms.DfExtensions.drop_columns">drop_columns</a></code></li>
<li><code><a title="df_transforms.DfExtensions.left_diff_schemas" href="#df_transforms.DfExtensions.left_diff_schemas">left_diff_schemas</a></code></li>
<li><code><a title="df_transforms.DfExtensions.rename_column" href="#df_transforms.DfExtensions.rename_column">rename_column</a></code></li>
<li><code><a title="df_transforms.DfExtensions.rename_columns" href="#df_transforms.DfExtensions.rename_columns">rename_columns</a></code></li>
<li><code><a title="df_transforms.DfExtensions.right_diff_schemas" href="#df_transforms.DfExtensions.right_diff_schemas">right_diff_schemas</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>